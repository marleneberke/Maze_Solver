library(datasets)
?USJudgeRatings
head(USJudgeRatings)
mean(USJudgeRatings$INTG)
GetAverage = function(DataList, indeces){
SubsetData = DataList[indeces]
AverageRating = mean(SubsetData)
return(AverageRating)
}
SimulationResults = boot(data=USJudgeRatings$INTG,statistic=GetAverage,R=10000)
boot.ci(SimulationResults,type="perc")
GetDifference = function(DataFrame, indeces){
SubsetData = DataFrame[indeces,]
Difference = mean(SubsetData$WRIT)-mean(SubsetData$ORAL)
return(Difference)
}
DataFrame[indeces,]
TestIndeces = c(1,1,3)
TestIndeces = c(1,1,3)
USJudgeRatings[TestIndeces,]
SimulationResults = boot(data=USJudgeRatings,statistic=GetDifference,R=10000)
boot.ci(SimulationResults,type="perc")
cor(USJudgeRatings$PREP,USJudgeRatings$DECI)
TestIndeces = c(1,1,3)
USJudgeRatings[TestIndeces,]
SimulationResults = boot(data=USJudgeRatings,statistic=GetDifference,R=10000)
boot.ci(SimulationResults,type="perc")
cor(USJudgeRatings$PREP,USJudgeRatings$DECI)
GetCorrelation = function(DataFrame, indeces){
SubsetData = DataFrame[indeces,]
r = cor(SubsetData$PREP,SubsetData$DECI)
return(r)
}
SimulationResults = boot(data=USJudgeRatings,statistic=GetCorrelation,R=10000)
boot.ci(SimulationResults,type="perc")
#Appendix
MyBoot = function(InputData, PersonsFunction){
SampleSize = length(Data) # Save how many observations are in the original experiment
# To simulate a replication, sample a set of indeces that
# correspond to which data points form the replication.
IndecesForReplication = sample(1:SampleSize, size=SampleSize, replace=TRUE)
# The function above took the list 1, 2, ..., SampleSize (first argument)
# and it created a new list of the same size (second argument)
# by sampling with replacement (third argument)!
# LEAP OF FAITH: Whoever called this function sent us another function as an argument. # let's hope they understand that that function is supposed to take a dataset,
# then a set of indeces, and it's supposed to return
# a key value from the replication (maybe the percentage, etc. we don't know what
# the person implemented in their function, and we don't need to know as long as we get # a value back):
ResampledResult = PersonsFunction(InputData,IndecesForReplication) return(ResampledResult)
}
#Appendix
MyBoot = function(InputData, PersonsFunction){
SampleSize = length(Data) # Save how many observations are in the original experiment
# To simulate a replication, sample a set of indeces that
# correspond to which data points form the replication.
IndecesForReplication = sample(1:SampleSize, size=SampleSize, replace=TRUE)
# The function above took the list 1, 2, ..., SampleSize (first argument)
# and it created a new list of the same size (second argument)
# by sampling with replacement (third argument)!
# LEAP OF FAITH: Whoever called this function sent us another function as an argument. # let's hope they understand that that function is supposed to take a dataset,
# then a set of indeces, and it's supposed to return
# a key value from the replication (maybe the percentage, etc. we don't know what
# the person implemented in their function, and we don't need to know as long as we get # a value back):
ResampledResult = PersonsFunction(InputData,IndecesForReplication)
return(ResampledResult)
}
MyBetterBoot = function(InputData, PersonsFunction, Replications){
# This line will run the function 'MyBoot' whatever number of times 'Replications' is, # and then save the results into a list.
ResampledResults = replicate(Replications,MyBoot(InputData, PersonsFunction))
# And we can send it back
return(ResampledResults)
}
boot
mean(c(1,2,3)
knitr::opts_chunk$set(echo = TRUE)
# Now load each of these libraries.
# This block of code evaluates but the output is excluded from your report.
library(ggplot2)
library(dplyr)
library(pastecs)
# Uncomment the next line
selfesteem <- read.csv("selfesteemstudy.csv")
# Take a peek at your data
head(selfesteem)
pnorm(-10)
pnorm(-10.58)
# Now load each of these libraries.
# This block of code evaluates but the output is excluded from your report.
library(ggplot2)
library(dplyr)
library(pastecs)
# Uncomment the next line
selfesteem <- read.csv("selfesteemstudy.csv")
# Take a peek at your data
head(selfesteem)
View(selfesteem)
# Uncomment and complete
N <- nrow(selfesteem)
selfesteem_mean <- mean(selfesteem$SELF_ESTEEM)
selfesteem_sd <- sd(selfesteem$SELF_ESTEEM)
selfesteem_se <- selfesteem_sd/sqrt(N)
N
selfesteem_mean
selfesteem_se
selfesteem_sd
# Uncomment and complete
upper_bound <- selfesteem_mean + 1.96*selfesteem_se
lower_bound <- selfesteem_mean - 1.96*selfesteem_se
# Uncomment to report
upper_bound
lower_bound
stat.desc(selfesteem$SELF_ESTEEM)
1.96*selfesteem_se
1.96*selfesteem_se
pnorm(0.995)
?pnorm
pnorm(0.5)
pnorm(99.5)
qnorm(0.995)
N
# Uncomment and complete
zscore <- (selfesteem_mean - 3.65)/selfesteem_se
# Uncomment to report
zscore
qnorm(0.975)
pnorm(1.959964)
pnrom(10.5)
pnorm(10.5)
pnorm(-10.5)
# Add your code next line
pnorm(10.5)
# Now load each of these libraries.
# This block of code evaluates but the output is excluded from your report.
library(ggplot2)
library(dplyr)
library(pastecs)
# Uncomment the next line
selfesteem <- read.csv("selfesteemstudy.csv")
# Take a peek at your data
head(selfesteem)
# Uncomment the next line
selfesteem <- read.csv("selfesteemstudy.csv")
# Take a peek at your data
head(selfesteem)
# Uncomment and complete
N <- nrow(selfesteem)
selfesteem_mean <- mean(selfesteem$SELF_ESTEEM)
selfesteem_sd <- sd(selfesteem$SELF_ESTEEM)
selfesteem_se <- selfesteem_sd/sqrt(N)
N
selfesteem_mean
selfesteem_sd
selfesteem_se
# Uncomment and complete
upper_bound <- selfesteem_mean + 1.96*selfesteem_se
lower_bound <- selfesteem_mean - 1.96*selfesteem_se
# Uncomment to report
upper_bound
lower_bound
stat.desc(selfesteem$SELF_ESTEEM)
1.96*selfesteem_se
qnorm(0.995)
pnorm(2.575829)
qnorm(0.995)
pnorm(-2.57)
pnorm(2.57)
# Uncomment and complete
zscore <- (selfesteem_mean - 3.65)/selfesteem_se
# Uncomment to report
zscore
pnorm(-10.58)
2*pnorm(-10.58)
pnorm(10.58)
1-pnorm(10.58)
2*(1-pnorm(abs(zscore)))
2*pnorm(-10.58)
?cor.test
x1 = c(1,2,3,4,5,6,7)
x2 = c(1,2,3,4,5,6,7)
cor.test(x1, x2, "less")
x2 = c(0,0,0,0,0,0,0)
cor.test(x1, x2, "less")
x2 = c(0,1,0,1,0,1,0)
cor.test(x1, x2, "less")
x2 = c(1,2,3,4,5,6,7)
cor.test(x1, x2, "two.sided")
x2 = c(-1-,2,-3,-4,-5,-6,-7)
x2 = c(-1,-2,-3,-4,-5,-6,-7)
cor.test(x1, x2, "two.sided")
sum(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1)
sum(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1, 26.2)
mean(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1, 26.2)
sd(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1, 26.2)
sd(c(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1, 26.2))
mean(c(2.1, 2.5, 1.1, 2.3, 3.0, 1.5, 1.7, 2.1, 26.2))
a = c(1, 2, 3)
b = c(1,1,1)
(a-b)
(a-b)**2
(a-b)^2
knitr::opts_chunk$set(echo = TRUE)
# Now load each of these libraries.
# This block of code evaluates but the output is excluded from your report.
library(ggplot2)
library(dplyr)
library(Hmisc)
library(rstatix)
# load your data
happiness.prob <- read.csv("happiness_probability.csv")
# take a peak at your data
head(happiness.prob)
# Uncomment and complete
myOneWayModel <- aov(Rating ~ factor(Years), data = happiness.prob)
summary(myOneWayModel)
# Uncomment and complete
ggplot(happiness.prob, aes(x = Years, y = Rating, color = Condition, group = Condition)) +
stat_summary(fun = mean, geom = "line") +
stat_summary(fun.data = mean_cl_boot,  geom = "pointrange")
# Uncomment and complete
happiness.prob %>%
group_by(Years, Condition) %>%
summarise(mean = mean(Rating), sd = sd(Rating))
# Uncomment and complete
myTwoWayModel <- aov(Rating ~ factor(Years) * factor(Condition), data = happiness.prob)
summary(myTwoWayModel)
# Uncomment and complete
partial_eta_squared(myTwoWayModel)
# Uncomment and copmlete
happiness.prob %>%
group_by(Years) %>%
t_test(Rating ~ Condition)
knitr::opts_chunk$set(echo = TRUE)
# load your data
context.learning <- read.csv("contextlearning.csv")
# take a peak at your data
head(context.learning)
# Uncomment and complete
ggplot(context.learning, aes(x=context_test, y=score, group=context_study, color=context_study)) +
stat_summary(fun = mean, geom = "line") +
stat_summary(fun.data = mean_cl_boot,  geom = "pointrange")
context.learning %>%
group_by(context_test, context_study) %>%
summarise(mean = mean(score), sd = sd(score))
context.learning %>%
group_by(context_test) %>%
summarise(mean = mean(score), sd = sd(score))
context.learning %>%
group_by(context_study) %>%
summarise(mean = mean(score), sd = sd(score))
# Uncomment and complete
myModel <- aov(score ~ factor(context_study) * factor(context_test), data = context.learning)
summary(myModel)
# Uncomment and complete
partial_eta_squared(myModel)
# Uncomment and complete
pairwise.t.test(context.learning$score, context.learning$context_study, p.adjust.method = "bonf")
?pairwise.t.test
# Uncomment and copmlete
context.learning %>%
group_by(context_study) %>%
t_test(score ~ context_test)
knitr::opts_chunk$set(echo = TRUE)
# load your data
coping.stress <- read.csv("coping_stress_aggregate.csv")
# take a peak at your data
head(coping.stress)
# Uncomment and complete
myModel <- aov(Q2_generalstress ~ factor(Q1_destress), data = coping.stress)
summary(myModel)
ggplot(temp, aes(Attempt5, participants1)) +
geom_point() +
#geom_smooth(method='lm') +
xlab("Model Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)
library(tidyverse)
library(ggplot2)
#######
#helper function
#code locations of pauses as before, at, or after intersection. can later
#use mutate to recode as at or not at an intersection
code_locations <- function(stim){
pause_location_coded <- vector(mode = "list", length = length(stim))
for (i in 1:length(stim)){
print(i)
if(str_detect(stim[i], "10_\\(10;2\\)")){
pause_location_coded[i] <- "after"
} else if(str_detect(stim[i], "10_\\(10;8\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "10_\\(6;1\\)")){
pause_location_coded[i] <- "before"
} else if(str_detect(stim[i], "10_\\(7;1\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(1;3\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(10;11\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(12;1\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(3;5\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(5;14\\)")){
pause_location_coded[i] <- "before"
} else if(str_detect(stim[i], "15_\\(5;15\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(6;7\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "15_\\(7;15\\)")){
pause_location_coded[i] <- "after"
} else if(str_detect(stim[i], "15_\\(7;5\\)")){
pause_location_coded[i] <- "after"
} else if(str_detect(stim[i], "15_\\(7;7\\)")){
pause_location_coded[i] <- "before"
} else if(str_detect(stim[i], "7_\\(4;1\\)")){
pause_location_coded[i] <- "before"
} else if(str_detect(stim[i], "7_\\(5;1\\)")){
pause_location_coded[i] <- "at"
} else if(str_detect(stim[i], "7_\\(7;1\\)")){
pause_location_coded[i] <- "after"
} else if(str_detect(stim[i], "7_\\(6;3\\)")){
pause_location_coded[i] <- "at"
}
}
return (pause_location_coded)
}
setwd("~/Documents/03_Yale/Projects/005_Thinking_About_Thinking/Code/Data_Analysis")
model_data <- read_csv("Model_Analysis/model.csv")
#participant_data <- read_csv("Participant_Analysis/participants_pilot1.csv")
# participant_data1 <- read_csv("Participant_Analysis/participants_pilot1.csv")
# participant_data2 <- read_csv("Participant_Analysis/participants_pilot2.csv")
participant_data <- read_csv("Participant_Analysis/participants_biglaunch_boot.csv")
participant_data <- participant_data %>% rename(scaled = mean)
participant_data <- participant_data %>% mutate(model_name = "participants1")
model_data <- model_data %>% mutate(stim = str_sub(maze_name, 1, 2)) %>%
mutate(stim = str_replace(stim, "_", "")) %>% mutate(stim = paste(stim, "_", sep = "")) %>%
mutate(stim = paste(stim, pause_location, sep = "")) %>% mutate(stim = paste(stim, "_", sep = "")) %>%
mutate(stim = paste(stim, pause_time, sep = ""))
all_data <- full_join(participant_data, model_data)
temp <- all_data %>% select(model_name, stim, scaled) %>%
spread(model_name, scaled)
temp <- drop_na(temp)
cor.test(temp$Attempt5, temp$participants1)#temp$Attempt5)
#plot model predictions against particpant data. line for x=y
ggplot(temp, aes(Attempt5, participants1)) + geom_point() +
geom_text(aes(label=stim),hjust=0, vjust=0)# +
#Which pauses have the least agreement between the model and participants? The most?
temp <- temp %>% mutate(difference = participants1 - Attempt5)
#cleaing
all_data <- all_data %>%
mutate(pretty_stim_name = paste("`", stim, sep="")) %>%
mutate(pretty_stim_name = paste(pretty_stim_name, "`", sep=""))
ggplot(all_data, aes(x = model_name, fill = model_name, y = scaled)) +
geom_bar(stat = "identity") +
facet_wrap(~pretty_stim_name)
all_data %>% ggplot(aes(pretty_stim_name, scaled, color = model_name)) + geom_point() +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) + facet_wrap(~pretty_stim_name) +
coord_flip()
all_data <- all_data %>% mutate(dummy = 1)
all_data %>% ggplot(aes(dummy, scaled, color = model_name)) + geom_point() +
geom_linerange(aes(ymin = ci_lower, ymax = ci_upper)) + facet_wrap(~pretty_stim_name) +
coord_flip()
#################################################################################
all_data$location_wrt_intersection <- code_locations(all_data$stim)
all_data <- as.data.frame(lapply(all_data, unlist)) #have to do because code location made a list
temp <- all_data %>% select(model_name, stim, scaled, location_wrt_intersection, pause_time) %>% spread(model_name, scaled)
cor.test(temp$Attempt5, temp$participants1)
ggplot(temp, aes(Attempt5, participants1)) +
geom_point(aes(color = location_wrt_intersection, size = pause_time)) +
scale_color_manual(values=c("#004c6d", "#6996b3", "#c1e7ff")) +
scale_size(range = c(1, 3)) +
geom_smooth(method='lm') +
xlab("Model Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)
ggplot(temp, aes(Attempt5, participants1)) +
geom_point() +
#geom_smooth(method='lm') +
xlab("Model Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)
ggplot(temp, aes(Attempt5, participants1)) +
geom_point() +
geom_smooth(method='lm') +
xlab("Model Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)
View(temp)
#################################################################################
#Try a cue-based regression
#Regression1: pause time
participants <- all_data %>% filter(model_name == "participants1")
output1 <- lm(participants$scaled ~ participants$pause_time)
summary(output1)
#Regression2: at intersection or not
participants$location_wrt_intersection <- code_locations(participants$stim)
participants <- participants %>% mutate(at_intersection = location_wrt_intersection=="at")
output2 <- lm(participants$scaled ~ participants$at_intersection)
summary(output2)
#Regression3: both pause time and intersection together
output3 <- lm(scaled ~ pause_time + at_intersection + pause_time:at_intersection, participants)
summary(output3)
#################################################################################
#Barplots with regression model predictions
predict(output1, )
ggplot(all_data, aes(x = model_name, fill = model_name, y = scaled)) +
geom_bar(stat = "identity") +
facet_wrap(~pretty_stim_name)
which_stim <- c("10_(6;1)_320", "10_(7;1)_320", "10_(10;2)_320", "15_(1;3)_320", "7_(5;1)_320")
subset <- all_data %>% filter(stim %in% which_stim)
which_indices = all_data$stim %in% which_stim
pause_lm = predict(output1)[which_indices[1:23]]
new_data_1 <-
data.frame(
model_name = rep("pause_lm", length(which_stim)),
scaled = pause_lm,
stim = subset$stim[1:length(which_stim)],
maze_name = subset$maze_name[1:length(which_stim)],
pause_location = subset$pause_location[1:length(which_stim)],
pause_time = subset$pause_time[1:length(which_stim)],
pretty_stim_name = subset$pretty_stim_name[1:length(which_stim)],
location_wrt_intersection = subset$location_wrt_intersection[1:length(which_stim)]
)
where_lm = predict(output2)[which_indices[1:23]]
new_data_2 <-
data.frame(
model_name = rep("where_lm", length(which_stim)),
scaled = where_lm,
stim = subset$stim[1:length(which_stim)],
maze_name = subset$maze_name[1:length(which_stim)],
pause_location = subset$pause_location[1:length(which_stim)],
pause_time = subset$pause_time[1:length(which_stim)],
pretty_stim_name = subset$pretty_stim_name[1:length(which_stim)],
location_wrt_intersection = subset$location_wrt_intersection[1:length(which_stim)]
)
regression_df <- full_join(new_data_1, new_data_2)
everything <- full_join(subset, regression_df)
ggplot(everything, aes(x = model_name, fill = model_name, y = scaled)) +
geom_bar(stat = "identity") +
facet_wrap(~pretty_stim_name) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
theme(aspect.ratio=1)
ggplot(subset, aes(x = model_name, fill = model_name, y = scaled)) +
geom_bar(stat = "identity") +
facet_wrap(~pretty_stim_name) +
geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) +
theme(aspect.ratio=1)
########################################################
#p values for comparing correlations
library(cocor)
pause_lm = predict(output1)
partic_data <- all_data %>% filter(model_name=="participants1")
model <- all_data %>% filter(model_name=="Attempt5")
new_data_1 <-
data.frame(
model_name = rep("pause_lm", length(pause_lm)),
scaled = pause_lm,
stim = partic_data$stim,
maze_name = partic_data$maze_name,
pause_location = partic_data$pause_location,
pause_time = partic_data$pause_time,
pretty_stim_name = partic_data$pretty_stim_name,
location_wrt_intersection = partic_data$location_wrt_intersection
)
where_lm <- predict(output2)
new_data_2 <-
data.frame(
model_name = rep("where_lm", length(where_lm)),
scaled = where_lm,
stim = partic_data$stim,
maze_name = partic_data$maze_name,
pause_location = partic_data$pause_location,
pause_time = partic_data$pause_time,
pretty_stim_name = partic_data$pretty_stim_name,
location_wrt_intersection = partic_data$location_wrt_intersection
)
both_cues_lm <- predict(output3)
new_data_3 <-
data.frame(
model_name = rep("both_cues_lm", length(both_cues_lm)),
scaled = both_cues_lm,
stim = partic_data$stim,
maze_name = partic_data$maze_name,
pause_location = partic_data$pause_location,
pause_time = partic_data$pause_time,
pretty_stim_name = partic_data$pretty_stim_name,
location_wrt_intersection = partic_data$location_wrt_intersection
)
partic_data <- partic_data %>% select(-n, -empirical_stat, -ci_upper, -ci_lower, -dummy)
model <- model %>% select(-mean_time_distracted, -time_thinking, -sd_time_distracted, -proportion_distracted, -dummy)
pause_lm_main_model <- full_join(partic_data, new_data_1)
everything <- full_join(pause_lm_main_model, new_data_2)
everything <- full_join(pause_lm_main_model, new_data_3)
everything <- full_join(everything, model)
everything <- everything  %>% spread(model_name, scaled)
cocor(~participants1 + Attempt5 | participants1 + pause_lm, everything)
cocor(~participants1 + Attempt5 | participants1 + where_lm, everything)
cor.test(everything$participants1, everything$where_lm)
########################################################
ggplot(everything, aes(both_cues_lm, participants1))+
geom_point() +
geom_smooth(method='lm') +
xlab("Both Cues Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)
########################################################
ggplot(everything, aes(both_cues_lm, participants1))+
geom_point() +
geom_smooth(method='lm') +
xlab("Both Cues Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1) +
ylim(c(-1,1.5))
ggplot(temp, aes(Attempt5, participants1)) +
geom_point() +
geom_smooth(method='lm') +
xlab("Model Predictions") + ylab("Participant Judgements") +
theme(aspect.ratio=1)  +
ylim(c(-1,1.5))
